{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h1>Imports</h1>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import re\n",
    "import nltk\n",
    "import gzip\n",
    "import shutil\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import backend\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, Dropout, LSTM, Bidirectional, TimeDistributed, Dense, Flatten\n",
    "from keras import layers\n",
    "from keras.layers.merge import concatenate\n",
    "from keras import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h1>Data preparation</h1>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if('generation-projet-trn.tar.gz' not in os.listdir()):\n",
    "    ! wget https://sophierosset.github.io/docs/1718/generation-projet-trn.tar.gz\n",
    "    ! wget https://sophierosset.github.io/docs/1718/generation-projet-dev.tar.gz\n",
    "    ! wget https://sophierosset.github.io/docs/1718/generation-projet-test.tar.gz\n",
    "\n",
    "    for file in os.listdir():\n",
    "        if(file.endswith(\"tar.gz\")):\n",
    "            tar = tarfile.open(file, \"r:gz\")\n",
    "            tar.extractall()\n",
    "            tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(filename):\n",
    "    l = []\n",
    "    for file in os.listdir(filename):\n",
    "        if(file.endswith(\"xml\")):\n",
    "            with open(os.path.join(filename,file)) as f:\n",
    "                l += f.read().split(\"\\n\")\n",
    "    \n",
    "    return [el for el in l if len(el) != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nbre of sentences in training set: 2772244\n",
      "nbre of sentences in validation set: 1839\n",
      "nbre of sentences in testing set: 2450\n"
     ]
    }
   ],
   "source": [
    "train = get_data(\"generation-projet-trn\")\n",
    "val = get_data(\"generation-projet-dev\")\n",
    "test = get_data(\"generation-projet-test\")\n",
    "print(\"nbre of sentences in training set: {}\\nnbre of sentences in \\\n",
    "validation set: {}\\nnbre of sentences in testing set: {}\"\\\n",
    "      .format(len(train),len(val),len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The set of tags in the xml files are: {'<ingredient>', '<neg_cat-ingredient>', '<neg_ingredient>', '<recipe>', '<cat-ingredient>'}\n"
     ]
    }
   ],
   "source": [
    "tags = set()\n",
    "l = re.findall('<[^/].*?>',str(train))\n",
    "for el in l:\n",
    "    tags.add(el)\n",
    "    \n",
    "closed_tags = set()\n",
    "l = re.findall('</.*?>',str(train))\n",
    "for el in l:\n",
    "    closed_tags.add(el)\n",
    "print(\"The set of tags in the xml files are: {}\".format(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2index  = {}\n",
    "index2tag = {}\n",
    "\n",
    "for i,tag in enumerate(tags):\n",
    "    tag2index[tag] = i+1\n",
    "    index2tag[i+1] = tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(l):\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for el in l:\n",
    "        tmp = []\n",
    "        tmp1 = []\n",
    "        tag = 0\n",
    "        \n",
    "        for w in el.split(\" \"):\n",
    "            if(w in tags or w in closed_tags):\n",
    "                if(w in tags):\n",
    "                    tag = tag2index[w]\n",
    "                else:\n",
    "                    tag = 0\n",
    "            else:\n",
    "                tmp.append(w)\n",
    "                tmp1.append(tag)\n",
    "        \n",
    "        X.append(tmp)\n",
    "        y.append(tmp1)\n",
    "        \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,y_train = prepare_data(train)\n",
    "X_val,y_val = prepare_data(val)\n",
    "X_test,y_test = prepare_data(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre maximal de mots dans un phrase:92\n",
      "nombre minimal de mots dans un phrase:2\n",
      "nombre moyenne de mots dans un phrase:8.5\n"
     ]
    }
   ],
   "source": [
    "lengths = [len(x) for x in X_train]\n",
    "print(\"nombre maximal de mots dans un phrase:{0}\\n\\\n",
    "nombre minimal de mots dans un phrase:{1}\\n\\\n",
    "nombre moyenne de mots dans un phrase:{2:.1f}\"\\\n",
    "      .format(np.max(lengths),np.min(lengths),np.mean(lengths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = np.max(lengths)\n",
    "max_words = 50000\n",
    "embedding_dim = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10725 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words = max_words)\n",
    "\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "sequences = np.array(tokenizer.texts_to_sequences(X_train))\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = preprocessing.sequence.pad_sequences(sequences, maxlen=maxlen)\n",
    "y_train = preprocessing.sequence.pad_sequences(y_train, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = np.array(tokenizer.texts_to_sequences(X_val))\n",
    "X_val = preprocessing.sequence.pad_sequences(sequences, maxlen=maxlen)\n",
    "y_val = preprocessing.sequence.pad_sequences(y_val, maxlen=maxlen)\n",
    "\n",
    "sequences = np.array(tokenizer.texts_to_sequences(X_test))\n",
    "X_test = preprocessing.sequence.pad_sequences(sequences, maxlen=maxlen)\n",
    "y_test = preprocessing.sequence.pad_sequences(y_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2772244, 92), (2772244, 92), (2450, 92), (2450, 92), (1839, 92), (1839, 92))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,y_train.shape,X_test.shape,y_test.shape,X_val.shape,y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h1>Pretrained word embeddings</h1>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if('output.txt' not in os.listdir()):\n",
    "    ! wget http://embeddings.net/frWac_non_lem_no_postag_no_phrase_200_cbow_cut0.bin\n",
    "    ! git clone https://github.com/marekrei/convertvec\n",
    "    os.chdir(os.path.join(os.getcwd(),\"convertvec\"))\n",
    "    ! make\n",
    "    ! ./convertvec bin2txt ../frWac_non_lem_no_postag_no_phrase_200_cbow_cut0.bin ../output.txt\n",
    "    os.chdir(\"/\".join(os.getcwd().split(\"/\")[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2516802 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "\n",
    "with open('output.txt','r',encoding='ISO-8859-1') as f:\n",
    "    for i,line in enumerate(f):\n",
    "        if(i != 0):\n",
    "            values = line.split()\n",
    "            word = \"\".join([values[i] for i in range(len(values) - 200)])\n",
    "\n",
    "            coefs = np.asarray(values[len(values) - 200:], dtype='float32')\n",
    "\n",
    "            embeddings_index[word] = coefs\n",
    "            \n",
    "\n",
    "embeddings_index[\"unk\"] = np.zeros(200)\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10725, 200)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_words = np.min([50000,len(word_index)])\n",
    "\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words:\n",
    "        \n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        \n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            embedding_matrix[i] = embeddings_index.get(\"unk\")\n",
    "\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 92, 200)           2145000   \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 92, 300)           1202400   \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 92, 300)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_8 (Bidirection (None, 92, 300)           1442400   \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 92, 300)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 92, 6)             1806      \n",
      "=================================================================\n",
      "Total params: 4,791,606\n",
      "Trainable params: 4,791,606\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_words , embedding_dim, weights = [embedding_matrix], input_length=maxlen))\n",
    "model.add(Bidirectional(LSTM(300, return_sequences=True),merge_mode='ave'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Bidirectional(LSTM(300, return_sequences=True),merge_mode='ave'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(TimeDistributed(Dense(6, activation = 'softmax'), input_shape=(92, 300)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = (np.arange(y_train.max()+1) == y_train[...,None]).astype(int)\n",
    "y_val = (np.arange(y_val.max()+1) == y_val[...,None]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile('adam', 'categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2772244 samples, validate on 1839 samples\n",
      "Epoch 1/1\n",
      "2772244/2772244 [==============================] - 21736s 8ms/step - loss: 7.6404e-04 - acc: 0.9998 - val_loss: 0.0772 - val_acc: 0.9933\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f11511a8e10>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train,\n",
    "          batch_size=256,\n",
    "          epochs=1,\n",
    "          validation_data=[X_val, y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9820984915705413"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(np.ravel(y_test), np.ravel(np.argmax(ypred,axis=2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
